<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" version="2.0"><channel><title>Governance, Value and Strategy Feed</title><link>https://surly-16.github.io/js-audio-feed/Governance,%20Value%20and%20Strategy/</link><description>Narrated episodes on Governance, Value and Strategy</description><language>en-au</language><itunes:explicit>no</itunes:explicit><itunes:image href="https://raw.githubusercontent.com/surly-16/js-audio-feed/refs/heads/main/Governance,%20Value%20and%20Strategy/Cover.png" /><image><url>https://raw.githubusercontent.com/surly-16/js-audio-feed/refs/heads/main/Governance,%20Value%20and%20Strategy/Cover.png</url><title>Governance, Value and Strategy Feed</title><link>https://surly-16.github.io/js-audio-feed/Governance,%20Value%20and%20Strategy/</link></image><item><title>009 Vendor Risk And Third Party Ai Management</title><guid>009_vendor_risk_and_third_party_ai_management</guid><pubDate>Sun, 15 Jun 2025 04:15:03 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Governance,%20Value%20and%20Strategy/media/009_vendor_risk_and_third_party_ai_management.mp3" length="5425965" type="audio/mpeg" /><description>Narrated episode: 009 Vendor Risk And Third Party Ai Management

Right now, your bank is probably evaluating or already using AI tools from multiple vendors. Maybe it's a document processing solution from one provider, a customer insights platform from another, and a fraud detection model from a third. Each vendor brings their own risks, their own governance requirements, and their own integration challenges. 

Here's what keeps me up at night as a Product Owner: we're building dependencies on black-box systems that our risk teams can't fully audit. When APRA drops CPS 230 requirements on us, or when a vendor's model drifts and starts flagging legitimate transactions as fraud, who owns that risk? 

Let me share what happened last quarter. We had a vendor's sentiment analysis tool suddenly start misclassifying customer complaints. The model had been retrained on US data without notification. Our complaints team spent three weeks manually reviewing cases before we caught it. That's the reality of third-party AI risk - it's not just about the initial assessment, it's about continuous monitoring. 

Think about your current vendor evaluation process. You probably have a checklist - data security, compliance certifications, SLAs. But are you asking about model interpretability? About drift detection? About how they handle edge cases specific to Australian banking? 

Here's where Lean Six Sigma thinking transforms this challenge. Instead of treating vendor AI as a procurement exercise, we need to treat it as a process control problem. Every third-party model is essentially an uncontrolled variable in your process. 

Let me break down our framework. First, we categorize vendors by risk tier. A marketing personalization tool sits in a different tier than a credit decisioning model. For high-risk tiers, we require what I call "glass box" provisions - the ability to inspect model logic, test with our own data, and monitor performance metrics in real-time. 

CommBank's AI Factory approach offers a blueprint here. They've built internal capabilities specifically to validate and monitor vendor models. When they onboard a new AI vendor, that model goes through their validation pipeline just like an internal model would. They're not just trusting vendor assertions - they're verifying. 

Now, let's talk about the Insight7 framework. They've identified seven critical evaluation criteria for AI vendors: model transparency, data governance, performance monitoring, bias testing, security controls, regulatory alignment, and operational integration. Most banks focus on maybe three of these. That's not enough anymore. 

Here's a practical example. We recently evaluated a customer churn prediction vendor. Their accuracy metrics looked great - 94% precision. But when we dug into their bias testing, we found the model performed poorly for customers over 65 and recent migrants. In the Australian market, that's not just a performance issue - it's a regulatory nightmare waiting to happen. 

The Protiviti framework adds another layer - third-party AI governance isn't just about the vendor, it's about your internal processes. Who in your organization can actually validate a vendor's claims about model fairness? Do you have the technical capability to monitor model drift? 

This is where the journey from RPA to AI becomes crucial. Your teams who built Excel macros and Access databases understand process logic. They can learn to ask the right questions about AI vendors. But they need frameworks and tools. 

Here's what we've implemented: a vendor AI risk scorecard that maps directly to APRA's CPS 234 and upcoming CPS 230 requirements. Each vendor gets scored on data lineage, model explainability, and incident response capabilities. But here's the key - we don't just score them once. We have automated monitoring that tracks their API response times, error rates, and output distributions. 

NContracts highlights ten critical tips, but I'll focus on the three that matter most in Australian banking. First, contractual right to audit. Not just their SOC reports - actual model audits. Second, clear escalation paths when models fail. And third, exit strategies that include data deletion and model transition plans. 

Let me share a win from last month. Our mortgage team was using a vendor's income verification model. Through our monitoring, we detected the model was struggling with gig economy income - increasingly common in Australia. Because we had the right contractual provisions, we worked with the vendor to retrain the model on Australian gig economy data. Result? 23% improvement in auto-approval rates for self-employed applicants. 

But here's where most banks stumble - they treat vendor management as a compliance exercise rather than a competitive advantage. Your third-party AI ecosystem should be as strategic as your internal capabilities. These vendors are extending your ability to serve customers and manage risk. 

Think about concentration risk differently. It's not just about having too many loans in one sector. It's about having too many critical processes dependent on one AI vendor. What happens if your document processing vendor has an outage during month-end? Or if your fraud detection vendor gets acquired and changes their pricing model? 

We've started mapping vendor dependencies like we map system dependencies. Each AI touchpoint in a customer journey gets tagged with its vendor. Suddenly, you can see that your home loan application process touches six different AI vendors. That's six potential points of failure, six different SLAs to manage, six different model governance frameworks to monitor. 

Here's a tactical approach we've borrowed from manufacturing - supplier scorecards. Every quarter, we score our AI vendors on performance, innovation, and risk. But unlike traditional IT vendors, we include metrics like model drift rate, false positive trends, and time-to-fix for edge cases. 

The innovation metric is crucial. Is your vendor improving their models? Are they adapting to Australian regulatory changes? We had one vendor whose model hadn't been updated in 18 months. In AI terms, that's ancient. We worked with them to establish a quarterly model refresh cycle, aligned with our own risk reviews. 

ASIC Report 798 makes it clear - you can't outsource accountability. When a vendor's AI makes a decision that impacts a customer, your bank owns that decision. This means your vendor management framework needs teeth. 

We've implemented what I call "kill switches" for high-risk vendor AI. If a model starts behaving erratically, our operations team can revert to manual processing within 15 minutes. It's not elegant, but it's saved us from several potential incidents. 

Let's talk about the build-versus-buy decision through a risk lens. When Westpac reports productivity gains from their AI initiatives, they're often combining internal capabilities with vendor solutions. The key is knowing which to own and which to outsource. 

Core differentiators - like your credit risk models - you probably want to own. Commodity capabilities - like document OCR - you can vendor. But there's a middle ground - strategic partnerships where you co-develop models with vendors. This gives you the speed of vendor deployment with the control of internal development. 

Here's where process improvement backgrounds shine. You understand variation, control limits, and process capability. Apply these concepts to vendor AI. What's the normal variation in model performance? When does variation become a control issue? How do you detect special cause variation in a black-box model? 

We've started requiring vendors to provide process control charts for their models. Weekly accuracy trends, monthly bias metrics, quarterly performance deep-dives. It's the same rigor we'd apply to any critical process, just adapted for AI. 

The change management aspect is often overlooked. When you introduce a new vendor AI tool, you're not just changing technology - you're changing how teams work. That credit analyst who used to manually review applications now needs to understand how to interpret AI recommendations and when to override them. 

We've found success with what I call "human-in-the-loop" vendor implementations. Even if the vendor's model can fully automate a decision, we start with human review. This builds trust, helps identify edge cases, and creates a feedback loop for model improvement. 

Looking at CPS 230 operational resilience requirements, third-party AI introduces new challenges. How do you ensure resilience when you don't control the model? We're working with vendors to implement redundancy at the model level - not just infrastructure redundancy, but actual alternative models that can take over if the primary fails. 

The economics matter too. AI vendor pricing models are evolving. Some charge per API call, others by data volume, some want revenue shares. We've learned to model total cost of ownership including the hidden costs - integration effort, monitoring overhead, and the cost of switching if needed. 

Here's a framework that's worked for us: treat vendor AI relationships like strategic partnerships, not transactional purchases. Regular business reviews, joint roadmap planning, shared KPIs. When your vendor succeeds, you succeed. When they fail, you have early warning. 

The data governance aspect is critical. Where is your customer data being processed? Is it leaving Australia? How is it being used to train the vendor's models? We've implemented data residency requirements and regular audits of vendor data practices. APRA is watching this space closely. 

Let me leave you with this: vendor AI management isn't about avoiding risk - it's about intelligent risk-taking. The banks that will win are those that can leverage vendor innovation while maintaining control and compliance. 

Start by auditing your current vendor AI landscape. Map every model, understand every dependency, document every risk. Then build the monitoring and governance frameworks to turn those vendors from risks into strategic assets. 

Your process improvement DNA is your advantage here. You know how to measure, monitor, and improve. Apply that rigor to vendor AI, and you'll be ahead of 90% of the market. The frameworks exist - Insight7, Protiviti, NContracts have done the thinking. Your job is to operationalize them in your unique context. 

Remember, in the world of third-party AI, trust is good, but verification is essential. Build the capabilities to verify, and you'll build the confidence to innovate.</description></item><item><title>005 Quick Wins Vs Strategic Ai Initiatives</title><guid>005_quick_wins_vs_strategic_ai_initiatives</guid><pubDate>Sun, 15 Jun 2025 04:14:27 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Governance,%20Value%20and%20Strategy/media/005_quick_wins_vs_strategic_ai_initiatives.mp3" length="6577773" type="audio/mpeg" /><description>Narrated episode: 005 Quick Wins Vs Strategic Ai Initiatives</description></item><item><title>008 Demonstrating Ai Value To Regulators And Stakeholders</title><guid>008_demonstrating_ai_value_to_regulators_and_stakeholders</guid><pubDate>Sun, 15 Jun 2025 04:14:15 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Governance,%20Value%20and%20Strategy/media/008_demonstrating_ai_value_to_regulators_and_stakeholders.mp3" length="4573101" type="audio/mpeg" /><description>Narrated episode: 008 Demonstrating Ai Value To Regulators And Stakeholders</description></item><item><title>006 Measuring Ai Roi Beyond Cost Savings</title><guid>006_measuring_ai_roi_beyond_cost_savings</guid><pubDate>Sun, 15 Jun 2025 04:12:28 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Governance,%20Value%20and%20Strategy/media/006_measuring_ai_roi_beyond_cost_savings.mp3" length="429357" type="audio/mpeg" /><description>Narrated episode: 006 Measuring Ai Roi Beyond Cost Savings</description></item><item><title>004 Risk Based Ai Roadmap Development</title><guid>004_risk_based_ai_roadmap_development</guid><pubDate>Sun, 15 Jun 2025 03:36:42 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Governance,%20Value%20and%20Strategy/media/004_risk_based_ai_roadmap_development.mp3" length="5001069" type="audio/mpeg" /><description>Narrated episode: 004 Risk Based Ai Roadmap Development</description></item><item><title>002 Ai Governance Gaps And Model Risk Management</title><guid>002_ai_governance_gaps_and_model_risk_management</guid><pubDate>Sun, 15 Jun 2025 03:36:03 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Governance,%20Value%20and%20Strategy/media/002_ai_governance_gaps_and_model_risk_management.mp3" length="4841709" type="audio/mpeg" /><description>Narrated episode: 002 Ai Governance Gaps And Model Risk Management</description></item><item><title>003 Building Compliant Ai Governance Frameworks</title><guid>003_building_compliant_ai_governance_frameworks</guid><pubDate>Sun, 15 Jun 2025 03:35:37 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Governance,%20Value%20and%20Strategy/media/003_building_compliant_ai_governance_frameworks.mp3" length="3450669" type="audio/mpeg" /><description>Narrated episode: 003 Building Compliant Ai Governance Frameworks</description></item><item><title>001 Navigating Australia S Ai Regulatory Framework</title><guid>001_navigating_australia_s_ai_regulatory_framework</guid><pubDate>Sun, 15 Jun 2025 03:34:59 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Governance,%20Value%20and%20Strategy/media/001_navigating_australia_s_ai_regulatory_framework.mp3" length="4447149" type="audio/mpeg" /><description>Narrated episode: 001 Navigating Australia S Ai Regulatory Framework</description></item></channel></rss>