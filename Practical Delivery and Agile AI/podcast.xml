<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" version="2.0"><channel><title>Practical Delivery and Agile AI Feed</title><link>https://surly-16.github.io/js-audio-feed/Practical%20Delivery%20and%20Agile%20AI/</link><description>Narrated episodes on Practical Delivery and Agile AI</description><language>en-au</language><itunes:explicit>no</itunes:explicit><itunes:image href="https://raw.githubusercontent.com/surly-16/js-audio-feed/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/Cover.png" /><image><url>https://raw.githubusercontent.com/surly-16/js-audio-feed/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/Cover.png</url><title>Practical Delivery and Agile AI Feed</title><link>https://surly-16.github.io/js-audio-feed/Practical%20Delivery%20and%20Agile%20AI/</link></image><item><title>008 Measuring Success In Early Stage Ai Squads</title><guid>008_measuring_success_in_early_stage_ai_squads</guid><pubDate>Sun, 15 Jun 2025 04:25:35 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/media/008_measuring_success_in_early_stage_ai_squads.mp3" length="5109549" type="audio/mpeg" /><description>Narrated episode: 008 Measuring Success In Early Stage Ai Squads

Right now, your AI squad is probably drowning in metrics that don't matter. I've watched teams track model accuracy to three decimal places while their business sponsors quietly pull funding because they can't see the value. Let me share what actually moves the needle when you're building AI capabilities in a regulated environment.



Here's the uncomfortable truth: traditional project metrics kill AI initiatives. When CBA launched their AI Factory, they didn't measure success by counting deployed models. They tracked how many manual decisions got automated with appropriate oversight. That's the shift we need to make.



Think about your last steering committee meeting. Did you present F1 scores and confusion matrices? Or did you show how many hours of manual review you eliminated while maintaining audit trails? The metrics that matter aren't technical—they're operational.



Let's start with what Finance Derivative taught us about measuring experimental ROI. They discovered that tracking "experiments per sprint" was meaningless. What mattered was "experiments that changed a business decision." One team ran fifty experiments in a quarter—impressive, right? Except none influenced actual trading strategies. Another team ran three experiments that fundamentally changed their risk modeling approach. Guess which team got renewed funding?



In our context at the bank, I've seen this play out repeatedly. A risk team built a beautiful anomaly detection model with ninety-eight percent accuracy. Fantastic technical achievement. But when we dug deeper, we found it flagged so many false positives that analysts started ignoring the alerts. The real metric we should've tracked? Percentage of alerts that led to actionable investigations.



SmartDev's approach to experimental ROI gives us a framework here. They measure three things: speed to first insight, cost per validated learning, and what they call "pivot velocity"—how quickly teams adjust based on results. Notice what's missing? Model performance metrics. They're measuring the learning process, not the output artifacts.



Let me give you a concrete example from our credit risk squad. Instead of tracking model accuracy, we started measuring "decision confidence lift." Before AI assistance, analysts spent forty percent of their time on low-confidence decisions—cases where they weren't sure which way to go. After implementing our classification model, that dropped to fifteen percent. That's a metric that resonates in the boardroom.



Grant Thornton's work on early-stage compliance metrics revealed something crucial for regulated environments. They found that "compliance by design" metrics outperformed "compliance by review" metrics by a factor of ten in terms of actual risk reduction. What does this mean practically? Track how many compliance requirements are built into your AI workflows versus how many require manual checking afterward.



Here's how we implemented this in our AML squad. We stopped counting false positive rates and started tracking "investigator productivity"—how many cases an analyst could thoroughly review per day. When our model pre-filtered and pre-enriched cases with relevant context, investigators went from reviewing eight cases daily to twenty-five. More importantly, the quality of investigations improved because analysts spent time on analysis, not data gathering.



Sutherland's research on moving past AI hype to value creation identified a pattern: successful teams measure outcomes, not outputs. They track business KPIs that existed before AI, then show how AI moves those needles. This is particularly powerful when dealing with skeptical stakeholders who've seen too many proof-of-concepts die in production.



Let's talk about stakeholder-specific metrics. Your risk committee doesn't care about your model's precision-recall curve. They care about regulatory breaches avoided, audit findings reduced, and control effectiveness improved. Frame your metrics in their language. When we deployed our document classification system, we didn't talk about ninety-five percent accuracy. We talked about reducing document review time from three days to thirty minutes while maintaining SOX compliance.



For APRA and ASIC reporting, you need what I call "defensive metrics"—measurements that prove you're not introducing new risks. Track model drift, but translate it to business impact. "Our model drift monitoring caught a two percent accuracy degradation" becomes "We prevented approximately fifty thousand dollars in potential misclassified transactions through proactive model monitoring."



Here's a framework I use with all my squads: Level one metrics prove the model works—accuracy, precision, recall. Level two metrics prove the solution works—processing time, error rates, user adoption. Level three metrics prove the business impact—cost savings, risk reduction, revenue enablement. Most teams stop at level one. Winners focus on level three while maintaining level one as hygiene factors.



Consider user adoption metrics carefully. A model with seventy percent accuracy that people actually use beats a ninety-five percent accurate model that sits on the shelf. We track "voluntary usage rate"—when users have a choice between the AI-assisted process and the manual process, which do they choose? If that number isn't above eighty percent after the honeymoon period, you have a problem that better algorithms won't solve.



Let me share a cautionary tale about vanity metrics. One team proudly reported processing a million documents per month with their NLP system. Impressive, until we realized ninety percent were duplicate submissions that should have been caught at ingestion. They were celebrating a symptom of upstream process failure. Always ask: "What behavior does this metric incentivize?"



For early-stage squads, I recommend the "North Star plus Three" approach. Choose one North Star metric that captures overall value delivery—maybe it's "hours saved per analyst per week" or "regulatory findings reduced." Then pick three supporting metrics that act as early warning systems: typically one for quality, one for adoption, and one for operational health.



Quality metrics in production differ from development. In dev, you care about test set performance. In production, you care about edge case handling. Track "graceful failure rate"—when your model encounters something unexpected, does it fail safely and alert appropriately? This is especially critical under CPS two-thirty operational risk requirements.



Adoption metrics should be behavioral, not just usage counts. Don't just track logins or API calls. Track complete workflows. How many users start and finish the AI-assisted process? Where do they drop off? What manual workarounds are they creating? These insights often reveal more about your solution's real-world effectiveness than any accuracy metric.



Operational health metrics keep you out of trouble. Monitor inference latency, but frame it as "percentage of requests meeting SLA." Track model retraining frequency, but present it as "days since last production model update" with clear thresholds for concern. These metrics build confidence with risk and compliance teams who worry about AI reliability.



Here's something most teams miss: measure what doesn't happen. Our fraud detection squad's biggest win wasn't catching more fraud—it was reducing false positives that annoyed good customers. We tracked "high-value customer friction events" and showed a sixty percent reduction. That metric got executive attention because it directly impacted customer satisfaction scores.



For regulatory reporting, create metrics that tell a story of continuous improvement and control. ASIC Report seven-ninety-eight emphasizes the importance of demonstrating ongoing monitoring and adjustment. Track "model performance reviews completed" and "improvement actions implemented." Show that you're not just deploying and forgetting.



Cost metrics need nuance in AI projects. Don't just track development costs—track total cost of ownership including ongoing monitoring, retraining, and support. But balance this against opportunity cost. What's the cost of not automating? What's the risk of human error in the current process? These comparisons often justify higher AI operational costs.



Let me give you a practical measurement framework we use. Week one to four: measure technical performance and deployment readiness. Month two to three: measure user adoption and process integration. Month four onward: measure business impact and ROI. This staged approach manages expectations and shows progress even when business metrics take time to materialize.



Don't forget about measuring team learning velocity. How quickly can your squad go from business problem to deployed solution? This meta-metric often predicts long-term success better than any individual project metric. Teams that can rapidly iterate and learn outperform teams that spend months perfecting their first model.



For stakeholder communication, create metric dashboards with progressive disclosure. Executive view: three to five business impact metrics with trend arrows. Manager view: add operational metrics and adoption indicators. Technical view: include model performance and system health metrics. Same data, different lenses.



Here's my challenge to you: Look at your current metrics. How many require explanation to a business stakeholder? If it's more than twenty percent, you're measuring the wrong things. Transform "model drift detected" into "risk of decision quality degradation." Transform "ninety-five percent accuracy" into "five percent of cases require human review." Make your metrics tell the business story.



Remember, in early-stage AI squads, your metrics should evolve rapidly. What matters in month one won't matter in month six. Build measurement flexibility into your operating rhythm. The teams that succeed are those that can pivot their success criteria as they learn what actually drives value in their specific context.



The ultimate test? If your AI squad disappeared tomorrow, would the metrics you track show a clear business impact? If not, you're measuring activity, not outcomes. And in a resource-constrained, compliance-focused environment like banking, outcomes are the only currency that matters.</description></item><item><title>004 Integrating Ai Initiatives With Risk System Workflows</title><guid>004_integrating_ai_initiatives_with_risk_system_workflows</guid><pubDate>Sun, 15 Jun 2025 04:24:57 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/media/004_integrating_ai_initiatives_with_risk_system_workflows.mp3" length="4869741" type="audio/mpeg" /><description>Narrated episode: 004 Integrating Ai Initiatives With Risk System Workflows</description></item><item><title>001 Running Ai Experiments In Risk Management Squads</title><guid>001_running_ai_experiments_in_risk_management_squads</guid><pubDate>Sun, 15 Jun 2025 04:13:09 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/media/001_running_ai_experiments_in_risk_management_squads.mp3" length="5599917" type="audio/mpeg" /><description>Narrated episode: 001 Running Ai Experiments In Risk Management Squads</description></item><item><title>003 Managing Ai Sprints And User Stories In Jira</title><guid>003_managing_ai_sprints_and_user_stories_in_jira</guid><pubDate>Sun, 15 Jun 2025 04:13:04 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/media/003_managing_ai_sprints_and_user_stories_in_jira.mp3" length="4816173" type="audio/mpeg" /><description>Narrated episode: 003 Managing Ai Sprints And User Stories In Jira</description></item><item><title>009 Stakeholder Updates For Experimental Ai Programs</title><guid>009_stakeholder_updates_for_experimental_ai_programs</guid><pubDate>Sun, 15 Jun 2025 04:07:24 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/media/009_stakeholder_updates_for_experimental_ai_programs.mp3" length="4774317" type="audio/mpeg" /><description>Narrated episode: 009 Stakeholder Updates For Experimental Ai Programs</description></item><item><title>006 Quarterly Epic Planning For Ai Innovation</title><guid>006_quarterly_epic_planning_for_ai_innovation</guid><pubDate>Sun, 15 Jun 2025 04:06:08 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/media/006_quarterly_epic_planning_for_ai_innovation.mp3" length="5077101" type="audio/mpeg" /><description>Narrated episode: 006 Quarterly Epic Planning For Ai Innovation</description></item><item><title>007 Building Mvp Culture In Risk Tech Teams</title><guid>007_building_mvp_culture_in_risk_tech_teams</guid><pubDate>Sun, 15 Jun 2025 04:05:53 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/media/007_building_mvp_culture_in_risk_tech_teams.mp3" length="4166061" type="audio/mpeg" /><description>Narrated episode: 007 Building Mvp Culture In Risk Tech Teams</description></item><item><title>002 From Prototype To Production Ai Pilot Pathways</title><guid>002_from_prototype_to_production_ai_pilot_pathways</guid><pubDate>Sun, 15 Jun 2025 04:05:07 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/media/002_from_prototype_to_production_ai_pilot_pathways.mp3" length="6890157" type="audio/mpeg" /><description>Narrated episode: 002 From Prototype To Production Ai Pilot Pathways</description></item><item><title>005 Confluence Documentation For Ai Projects</title><guid>005_confluence_documentation_for_ai_projects</guid><pubDate>Sun, 15 Jun 2025 04:04:59 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/media/005_confluence_documentation_for_ai_projects.mp3" length="3947949" type="audio/mpeg" /><description>Narrated episode: 005 Confluence Documentation For Ai Projects</description></item></channel></rss>