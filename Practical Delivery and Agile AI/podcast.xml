<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" version="2.0"><channel><title>Practical Delivery and Agile AI Feed</title><link>https://surly-16.github.io/js-audio-feed/Practical%20Delivery%20and%20Agile%20AI/</link><description>Narrated episodes on Practical Delivery and Agile AI</description><language>en-au</language><itunes:explicit>no</itunes:explicit><itunes:image href="https://raw.githubusercontent.com/surly-16/js-audio-feed/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/cover.png?raw=true" /><image><url>https://raw.githubusercontent.com/surly-16/js-audio-feed/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/cover.png?raw=true</url><title>Practical Delivery and Agile AI Feed</title><link>https://surly-16.github.io/js-audio-feed/Practical%20Delivery%20and%20Agile%20AI/</link></image><item><title>006 Quarterly Epic Planning For Ai Innovation</title><guid>006_quarterly_epic_planning_for_ai_innovation</guid><pubDate>Sun, 15 Jun 2025 03:48:14 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/media/006_quarterly_epic_planning_for_ai_innovation.mp3" length="5860077" type="audio/mpeg" /><description>Narrated episode: 006 Quarterly Epic Planning For Ai Innovation

Right now, your AI initiatives are probably scattered across teams, each solving their own problems in isolation. Maybe Risk is building their own credit models while Marketing experiments with customer segmentation, and Operations automates document processing. Sound familiar? That fragmentation is killing your ability to scale AI impact across the bank. Today, I'm walking you through how to run quarterly epic planning that transforms these disconnected experiments into a cohesive AI innovation engine.



Let me paint you a picture of what good looks like. Commonwealth Bank's AI Factory didn't happen by accident. They moved from 150 disconnected AI experiments to a unified platform serving over 600 use cases. The secret? They stopped treating AI projects like traditional IT deliverables and started managing them as a portfolio of experiments with shared infrastructure and governance.



Here's the framework I've refined over the last eighteen months leading AI initiatives. First principle: your quarterly planning needs three distinct tracks. Track one is your foundation layer—the boring stuff that makes everything else possible. Think feature stores, model registries, monitoring dashboards. Track two is your rapid experimentation pipeline—low-risk, high-learning initiatives that can fail fast. Track three is your production scaling efforts—taking proven experiments and hardening them for enterprise deployment.



Let's dig into how this works in practice. Start your quarterly planning two weeks before the quarter ends. Not with a blank canvas, but with data. Pull metrics from your current experiments: inference volumes, accuracy drift, business impact measurements. If you're not tracking these yet, that's your first epic right there. You can't optimize what you don't measure.



Now, here's where most teams stumble. They jump straight into solution mode. "We need a fraud detection model!" No. Start with process performance gaps. What's actually broken? Where are your teams spending 80% of their time on repetitive analysis? Which compliance checks take three days but add minimal risk reduction? Map these pain points to measurable outcomes.



I learned this the hard way when we tried implementing an AI-powered credit assessment tool. The model was technically brilliant—95% accuracy, sub-second predictions. But it failed spectacularly because we didn't map the end-to-end process first. Turns out, the bottleneck wasn't the credit decision itself but the document verification step that happened two stages earlier. Six months of work solving the wrong problem.




So here's your planning sequence. Week minus two: gather process performance data and identify top friction points. Week minus one: run discovery workshops with process owners—not just tech teams. Day one of planning: present a prioritized backlog of problems, not solutions. Day two: match problems to AI capabilities with clear success metrics. Day three: assign epics to your three tracks with resource allocation.




Let's talk about epic structure for AI initiatives. Traditional user stories don't work here. Instead, each epic needs five components. One: the process baseline—current state metrics. Two: the hypothesis—what specific improvement AI will deliver. Three: the experiment design—your minimum viable model. Four: the governance requirements—which CPS 230 controls apply. Five: the scaling criteria—what triggers production deployment.



Here's a real example from last quarter. Epic: "Reduce false positive rate in transaction monitoring." Baseline: 94% false positive rate, 12 FTE hours daily on manual review. Hypothesis: ML model trained on historical investigation outcomes can reduce false positives by 40% while maintaining 100% true positive capture. Experiment: Train gradient boosting model on 24 months of labeled transactions, test on holdout set. Governance: Ensure model explanations meet AUSTRAC reporting requirements. Scaling criteria: Achieve 40% reduction in pilot, maintain performance for 30 days.



Notice what's not in there? No mention of specific algorithms, cloud platforms, or technical architecture. Those are implementation details that emerge during sprint planning. Your quarterly epics should focus on business outcomes and risk boundaries.



Now, the uncomfortable truth about AI planning in regulated environments. Half your effort will go into governance and monitoring. Accept it. Plan for it. When Westpac rolled out their customer service AI, they spent more time on bias testing and audit trails than on model development. That's not inefficiency—that's maturity.




Build governance checkpoints into your epic structure. Week four: data quality assessment and privacy impact review. Week eight: model bias testing and fairness metrics. Week twelve: operational risk assessment and control testing. These aren't optional add-ons—they're core deliverables that determine whether your AI actually makes it to production.




Let's address the elephant in the room: stakeholder management. Your quarterly planning session will have three types of participants. The enthusiasts who want to AI-ify everything—contain their enthusiasm with clear prioritization criteria. The skeptics who've seen too many failed pilots—win them over with small, measurable wins in their domain. The compliance folks who see only risks—make them your partners by building controls into every epic from day one.



Here's a technique that's transformed our planning sessions. Instead of presenting AI capabilities and asking "what could we do with this?", present process metrics and ask "what would have to be true for AI to improve this by 30%?" This flips the conversation from technology push to business pull.



Real example: our mortgage processing team was drowning in document verification. Instead of pitching computer vision, we asked: "What would need to be true to reduce document review time from 45 minutes to 15 minutes per application?" That question surfaced the real constraints—not OCR accuracy, but the ability to cross-reference extracted data with multiple source systems and maintain an audit trail. The epic became about building an integrated verification pipeline, not just a document reading model.




Your resource allocation strategy needs to reflect AI's experimental nature. Traditional IT projects front-load analysis and back-load development. AI initiatives need continuous iteration. Allocate resources in two-week sprints with clear experiment cycles. Sprint one: data exploration and baseline establishment. Sprint two: initial model development. Sprint three: evaluation and iteration. Sprint four: governance review and pivot decision.




Don't fall into the trap of planning features. Plan capabilities. Not "implement chatbot for customer service" but "build conversational AI platform with multi-channel deployment." Not "create fraud detection model" but "establish real-time anomaly detection framework." This capability-based planning lets you reuse infrastructure across use cases.



Here's where the Lean Six Sigma background pays dividends. Apply DMAIC thinking to your AI epics. Define: what process are we improving? Measure: what's our current performance? Analyze: why is AI the right solution? Improve: how do we experiment safely? Control: how do we monitor production performance? This framework translates perfectly to AI initiatives while keeping business stakeholders comfortable.



Let's talk velocity planning for AI teams. Traditional story points don't work when half your sprints might end with "the data doesn't support this hypothesis." Instead, measure experimental velocity—how many hypotheses can you test per quarter? Track learning efficiency—how quickly can you determine if an approach will work? Monitor technical debt—how much rework does production deployment require?



Your quarterly roadmap should tell a story. Quarter one: establish foundations and quick wins. Quarter two: scale successful experiments and tackle harder problems. Quarter three: integrate solutions and optimize performance. Quarter four: institutionalize capabilities and plan next year's transformation. Each quarter builds on the last, creating momentum.



The integration challenge is real. Your AI epics don't exist in isolation—they need to align with existing transformation programs. When ANZ implemented their AI-powered risk models, they had to coordinate with three separate regulatory change programs, two system upgrades, and a major process redesign. Build these dependencies into your planning from day one.




Here's a planning anti-pattern I see constantly: teams allocate 100% of their capacity to new initiatives. Wrong. Reserve 30% for model maintenance, monitoring, and retraining. That exciting fraud model you deployed last quarter? Its performance is already degrading. Plan capacity for continuous improvement or watch your AI initiatives turn into technical debt.




Let me share the most powerful question you can ask during epic planning: "What would we need to believe for this to deliver 10x impact instead of 10% improvement?" This forces thinking beyond incremental automation to true transformation. Maybe it's not about making the current process faster but eliminating steps entirely. Maybe it's not about better predictions but changing decision timing.



The skills gap is your hidden planning constraint. That ambitious NLP epic for contract analysis? Do you have anyone who understands transformer architectures? The real-time recommendation engine? Who knows about feature stores and streaming inference? Build learning and hiring into your quarterly plans or watch epics fail for lack of expertise.



Finally, your planning artifact shouldn't be a Gantt chart or a spreadsheet. Create a visual portfolio view. X-axis: implementation complexity. Y-axis: business impact. Bubble size: resource requirements. Color: regulatory risk level. This visualization makes trade-offs clear and helps stakeholders understand why you're prioritizing certain initiatives.



Remember, quarterly planning for AI isn't about predicting the future—it's about creating a framework for rapid learning and adaptation. Your plan will change. Experiments will fail. New opportunities will emerge. Build flexibility into your process, not rigidity. The banks winning with AI aren't the ones with perfect plans. They're the ones who can learn and pivot faster than their competitors while maintaining governance and control.



Next quarter, when you're sitting in that planning room, resist the urge to start with solutions. Start with processes, pain points, and performance gaps. Build your AI portfolio like a venture capitalist—some safe bets, some moderate risks, a few moonshots. Measure learning as much as delivery. And always, always keep one eye on production stability while the other looks for transformation opportunities. That's how you turn quarterly planning from a bureaucratic exercise into a strategic weapon for AI-driven innovation.</description></item><item><title>009 Stakeholder Updates For Experimental Ai Programs</title><guid>009_stakeholder_updates_for_experimental_ai_programs</guid><pubDate>Sun, 15 Jun 2025 03:33:59 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/media/009_stakeholder_updates_for_experimental_ai_programs.mp3" length="5770413" type="audio/mpeg" /><description>Narrated episode: 009 Stakeholder Updates For Experimental Ai Programs</description></item><item><title>003 Managing Ai Sprints And User Stories In Jira</title><guid>003_managing_ai_sprints_and_user_stories_in_jira</guid><pubDate>Sun, 15 Jun 2025 03:31:49 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/media/003_managing_ai_sprints_and_user_stories_in_jira.mp3" length="4085229" type="audio/mpeg" /><description>Narrated episode: 003 Managing Ai Sprints And User Stories In Jira</description></item><item><title>008 Measuring Success In Early Stage Ai Squads</title><guid>008_measuring_success_in_early_stage_ai_squads</guid><pubDate>Sun, 15 Jun 2025 03:13:16 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/media/008_measuring_success_in_early_stage_ai_squads.mp3" length="6828525" type="audio/mpeg" /><description>Narrated episode: 008 Measuring Success In Early Stage Ai Squads</description></item><item><title>007 Building Mvp Culture In Risk Tech Teams</title><guid>007_building_mvp_culture_in_risk_tech_teams</guid><pubDate>Sun, 15 Jun 2025 03:11:28 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/media/007_building_mvp_culture_in_risk_tech_teams.mp3" length="5202093" type="audio/mpeg" /><description>Narrated episode: 007 Building Mvp Culture In Risk Tech Teams</description></item><item><title>005 Confluence Documentation For Ai Projects</title><guid>005_confluence_documentation_for_ai_projects</guid><pubDate>Sun, 15 Jun 2025 03:10:35 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/media/005_confluence_documentation_for_ai_projects.mp3" length="4729773" type="audio/mpeg" /><description>Narrated episode: 005 Confluence Documentation For Ai Projects</description></item><item><title>002 From Prototype To Production Ai Pilot Pathways</title><guid>002_from_prototype_to_production_ai_pilot_pathways</guid><pubDate>Sun, 15 Jun 2025 03:10:03 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/media/002_from_prototype_to_production_ai_pilot_pathways.mp3" length="6759021" type="audio/mpeg" /><description>Narrated episode: 002 From Prototype To Production Ai Pilot Pathways</description></item><item><title>004 Integrating Ai Initiatives With Risk System Workflows</title><guid>004_integrating_ai_initiatives_with_risk_system_workflows</guid><pubDate>Sun, 15 Jun 2025 03:10:01 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/media/004_integrating_ai_initiatives_with_risk_system_workflows.mp3" length="5017773" type="audio/mpeg" /><description>Narrated episode: 004 Integrating Ai Initiatives With Risk System Workflows</description></item><item><title>001 Running Ai Experiments In Risk Management Squads</title><guid>001_running_ai_experiments_in_risk_management_squads</guid><pubDate>Sun, 15 Jun 2025 03:08:48 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/Practical%20Delivery%20and%20Agile%20AI/media/001_running_ai_experiments_in_risk_management_squads.mp3" length="5455725" type="audio/mpeg" /><description>Narrated episode: 001 Running Ai Experiments In Risk Management Squads</description></item></channel></rss>