<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" version="2.0"><channel><title>AI Foundations and Implementation Feed</title><link>https://surly-16.github.io/js-audio-feed/AI%20Foundations%20and%20Implementation/</link><description>Narrated episodes on AI Foundations and Implementation</description><language>en-au</language><itunes:explicit>no</itunes:explicit><itunes:image href="https://raw.githubusercontent.com/surly-16/js-audio-feed/refs/heads/main/AI%20Foundations%20and%20Implementation/Cover.png" /><image><url>https://raw.githubusercontent.com/surly-16/js-audio-feed/refs/heads/main/AI%20Foundations%20and%20Implementation/Cover.png</url><title>AI Foundations and Implementation Feed</title><link>https://surly-16.github.io/js-audio-feed/AI%20Foundations%20and%20Implementation/</link></image><item><title>009 Generative Ai And Agentic Systems In Risk Management</title><guid>009_generative_ai_and_agentic_systems_in_risk_management</guid><pubDate>Sun, 15 Jun 2025 04:08:35 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/AI%20Foundations%20and%20Implementation/media/009_generative_ai_and_agentic_systems_in_risk_management.mp3" length="6349293" type="audio/mpeg" /><description>Narrated episode: 009 Generative Ai And Agentic Systems In Risk Management

Right now, across every major Australian bank, there's a fundamental shift happening in how we manage risk. We're moving from reactive reporting to predictive intelligence, from manual reviews to autonomous agents that can process thousands of documents in minutes while maintaining full audit trails. 

Let me paint you a picture of what this looks like in practice. Last quarter, our credit risk team was drowning in covenant monitoring across our commercial lending portfolio. Traditionally, this meant analysts manually reviewing financial statements, checking ratios, flagging breaches. Classic high-volume, rules-based work that screams for automation. 

But here's where it gets interesting. Instead of building another RPA bot or VBA macro, we deployed an agentic AI system. Not just a chatbot or a document classifier, but a full autonomous workflow that ingests financial statements, extracts key metrics, calculates covenant compliance, generates exception reports, and even drafts initial breach notifications. All while maintaining complete lineage for APRA's CPS 230 operational resilience requirements. 

The results? Ninety-eight percent reduction in manual review time. But more importantly, we caught covenant breaches an average of twelve days earlier. That's twelve days of additional runway for workout strategies, twelve days less exposure for the bank. 

Now, when I say "agentic AI," I'm talking about systems that don't just respond to prompts or classify documents. These are autonomous agents that plan, execute, and adapt. Think of them as digital workers with specific roles and responsibilities. 

McKinsey's recent analysis shows banks using generative AI for risk management are seeing forty to sixty percent productivity gains. But here's what they're not telling you: the real value isn't in the efficiency metrics. It's in the risk reduction. 


Let me break down how these systems actually work in a regulated environment. First, you need to understand the architecture. We're not talking about a single large language model doing everything. It's an orchestrated system of specialized agents, each with defined capabilities and constraints.
 

Take our credit memo analysis agent. It doesn't just read documents. It maintains a structured knowledge graph of customer relationships, understands our credit policy framework, can access real-time market data, and knows when to escalate to human review. Every decision point is logged, every data source tracked. 

This is critical for ASIC Report 798 compliance around AI governance. We can demonstrate exactly how each risk decision was made, what data was considered, which model version was used. Full explainability, full auditability. 

But here's where most implementations fail: they try to automate existing processes instead of reimagining them. Classic mistake from the Lean Six Sigma playbook. You don't automate waste; you eliminate it first. 

Agentic AI lets us completely redesign risk workflows. Instead of sequential reviews and approvals, we can run parallel analysis streams. Instead of periodic reporting, we get continuous monitoring. Instead of sampling, we get full population testing. 

Let me give you a concrete example from our market risk team. Traditional VaR calculations ran overnight, delivered as static reports each morning. Our agentic system now runs continuous scenario analysis, automatically adjusting for market conditions, regulatory changes, even news events. 

When Silicon Valley Bank collapsed, our system had already flagged similar duration risk exposures in our portfolio and generated hedging recommendations before the first human analyst arrived at work. That's the power of autonomous agents: they don't sleep, they don't take breaks, and they're always watching. 

Now, I know what you're thinking. How do we trust these systems with critical risk decisions? This is where our process improvement background becomes invaluable. We apply the same rigor we used for Six Sigma implementations: clear process definitions, measurement systems, control mechanisms. 


Every agent operates within defined parameters. Think of it like control limits in statistical process control. The agent has autonomy within those limits, but any deviation triggers human review. We call it "guided autonomy"â€”the system can act independently for routine decisions but knows when to escalate.
 

CBA's AI Factory pioneered this approach with their customer complaint handling system. Agents process initial complaints, categorize issues, draft responses, but complex cases automatically route to specialists. The result? Fifty percent faster resolution times while maintaining higher customer satisfaction scores. 

But implementing these systems isn't just a technical challenge. It's a change management exercise. Your risk analysts aren't being replaced; they're being elevated. Instead of checking spreadsheets, they're training agents. Instead of writing reports, they're designing decision frameworks. 

This is where many banks stumble. They deploy the technology but don't transform the operating model. You need to rethink roles, responsibilities, even performance metrics. How do you measure a risk analyst whose primary job is now supervising AI agents? 

The answer lies in outcome-based metrics. Not how many reviews completed, but how many risks identified early. Not how many reports generated, but how many losses prevented. We're shifting from activity measures to impact measures. 

Let's talk about the technical architecture for a moment. Most of you are familiar with traditional ML pipelines: data ingestion, feature engineering, model training, deployment. Agentic systems add another layer: the orchestration layer. 

This is where tools like LangChain, AutoGen, or CrewAI come in. They provide frameworks for coordinating multiple agents, managing state, handling errors. But here's the key: you need to build this on a solid data foundation. 


Think of it this way: your agents are only as good as the data they can access. If your risk data is siloed in Excel files on shared drives, no amount of AI wizardry will help. You need centralized data lakes, real-time streaming pipelines, proper data governance.
 

Westpac learned this the hard way. Their initial AI pilots showed amazing results in test environments but failed in production because the data quality wasn't there. They had to step back, fix the foundations, then rebuild. Don't make the same mistake. 

Now, let's address the elephant in the room: hallucinations and reliability. Yes, large language models can generate incorrect information. But in agentic systems, we have multiple safeguards. 

First, we use retrieval-augmented generation. Agents don't rely on training data alone; they actively query verified sources. Second, we implement multi-agent verification. Critical decisions require consensus from multiple agents using different approaches. Third, we maintain human-in-the-loop for high-stakes decisions. 

Harvard Business Review's recent study on AI process redesign found that hybrid human-AI teams outperform either alone by thirty to forty percent. The key is designing the handoff points correctly. When should the agent escalate? How much context does the human need? How do we capture human decisions to improve the agent? 

This is where your process improvement skills become invaluable. You know how to map processes, identify decision points, design control mechanisms. Apply those same skills to agent design. 

Let me share a framework we use. We call it the "Trust Ladder." Level one: agents handle data collection and initial analysis. Level two: agents make recommendations with human approval. Level three: agents execute routine decisions with post-hoc review. Level four: full autonomy within defined parameters. 

Most risk processes start at level one or two. As you build confidence and refine the agents, you climb the ladder. Our covenant monitoring system took six months to reach level three. Our fraud detection agents reached level four in just three months because the feedback loops were tighter. 


The key is starting small and scaling systematically. Pick a well-defined process with clear rules and measurable outcomes. Build your first agent, measure performance, iterate. Don't try to automate your entire risk framework on day one.
 

FTI Consulting's research on cracking the AI code in banking found that successful implementations share three characteristics: clear business objectives, strong governance frameworks, and iterative deployment approaches. Notice what's not on that list? The latest model or the fanciest tech stack. 

Success comes from execution discipline, not technical sophistication. A well-implemented GPT-3.5 agent will outperform a poorly deployed GPT-4 system every time. Focus on the fundamentals: data quality, process design, change management. 

Let's talk about regulatory considerations. APRA's CPS 230 requires banks to maintain operational resilience. That means your AI systems need the same controls as any critical infrastructure: redundancy, failover mechanisms, disaster recovery plans. 

But it goes beyond technical resilience. You need governance structures, accountability frameworks, audit trails. Who's responsible when an agent makes a decision? How do you investigate issues? How do you ensure compliance with fair lending laws when algorithms are making credit decisions? 

The answer is transparency by design. Every agent decision must be explainable, traceable, and auditable. This isn't just good practice; it's regulatory requirement. 

We implement this through decision logs that capture not just what the agent decided, but why. What data was considered? What rules were applied? What confidence level was assigned? This creates an audit trail that satisfies both internal risk management and external regulatory review. 

Now, let's get practical. Where should you start? Based on our experience and industry benchmarks, here are the highest-impact use cases for agentic AI in risk management. 

First, document-heavy processes. Covenant monitoring, as I mentioned. Financial spreading for credit analysis. Regulatory report generation. These processes involve extracting data from unstructured documents, applying rules, generating outputs. Perfect for AI agents. 

Second, continuous monitoring tasks. Market risk surveillance, transaction monitoring for AML, cyber threat detection. Agents excel at watching multiple data streams simultaneously and flagging anomalies. 


Third, scenario analysis and stress testing. Instead of running quarterly stress tests, agents can continuously simulate thousands of scenarios, updating as market conditions change. This gives you real-time insight into portfolio vulnerabilities.
 

But here's what not to automate, at least not initially. Anything involving significant judgment calls, customer-facing decisions with fairness implications, or processes without clear success metrics. Build trust with straightforward use cases first. 

Let me leave you with this thought. We're at an inflection point in risk management. The banks that successfully deploy agentic AI won't just be more efficient; they'll be fundamentally more resilient. 

They'll catch risks earlier, respond faster, and make better decisions. But success requires more than technology. It requires reimagining how risk management works, transforming operating models, and building new capabilities. 

The question isn't whether to adopt agentic AI for risk management. The question is how fast you can do it responsibly. Your competitors are already moving. The regulators are watching. The technology is ready. 

Start small, think big, move fast. Pick your first use case, build your first agent, learn and iterate. The journey from rule-based automation to intelligent agents isn't just an upgradeâ€”it's a transformation. And in risk management, transformation isn't optional anymore. It's survival.</description></item><item><title>008 Ai Powered Compliance And Control Monitoring</title><guid>008_ai_powered_compliance_and_control_monitoring</guid><pubDate>Sun, 15 Jun 2025 03:35:32 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/AI%20Foundations%20and%20Implementation/media/008_ai_powered_compliance_and_control_monitoring.mp3" length="6625773" type="audio/mpeg" /><description>Narrated episode: 008 Ai Powered Compliance And Control Monitoring</description></item><item><title>007 Real Time Fraud Detection And Monitoring Systems</title><guid>007_real_time_fraud_detection_and_monitoring_systems</guid><pubDate>Sun, 15 Jun 2025 03:34:00 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/AI%20Foundations%20and%20Implementation/media/007_real_time_fraud_detection_and_monitoring_systems.mp3" length="5138157" type="audio/mpeg" /><description>Narrated episode: 007 Real Time Fraud Detection And Monitoring Systems</description></item><item><title>006 Ai Opportunity Mapping Using Process Improvement</title><guid>006_ai_opportunity_mapping_using_process_improvement</guid><pubDate>Sun, 15 Jun 2025 03:13:09 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/AI%20Foundations%20and%20Implementation/media/006_ai_opportunity_mapping_using_process_improvement.mp3" length="5191725" type="audio/mpeg" /><description>Narrated episode: 006 Ai Opportunity Mapping Using Process Improvement</description></item><item><title>005 Data Governance And Infrastructure Strategy</title><guid>005_data_governance_and_infrastructure_strategy</guid><pubDate>Sun, 15 Jun 2025 03:01:32 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/AI%20Foundations%20and%20Implementation/media/005_data_governance_and_infrastructure_strategy.mp3" length="5632173" type="audio/mpeg" /><description>Narrated episode: 005 Data Governance And Infrastructure Strategy</description></item><item><title>001 Ai Applications Across Banking Risk Functions</title><guid>001_ai_applications_across_banking_risk_functions</guid><pubDate>Sun, 15 Jun 2025 02:28:13 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/AI%20Foundations%20and%20Implementation/media/001_ai_applications_across_banking_risk_functions.mp3" length="6342573" type="audio/mpeg" /><description>Narrated episode: 001 Ai Applications Across Banking Risk Functions</description></item><item><title>003 Mlops For Banking Environments</title><guid>003_mlops_for_banking_environments</guid><pubDate>Sun, 15 Jun 2025 02:28:13 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/AI%20Foundations%20and%20Implementation/media/003_mlops_for_banking_environments.mp3" length="5836269" type="audio/mpeg" /><description>Narrated episode: 003 Mlops For Banking Environments</description></item><item><title>002 From Rules To Learning Rpa Vs Ai Automation</title><guid>002_from_rules_to_learning_rpa_vs_ai_automation</guid><pubDate>Sun, 15 Jun 2025 02:27:32 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/AI%20Foundations%20and%20Implementation/media/002_from_rules_to_learning_rpa_vs_ai_automation.mp3" length="4930029" type="audio/mpeg" /><description>Narrated episode: 002 From Rules To Learning Rpa Vs Ai Automation</description></item><item><title>004 Ai Explainability And Model Monitoring</title><guid>004_ai_explainability_and_model_monitoring</guid><pubDate>Sun, 15 Jun 2025 02:27:12 +1000</pubDate><enclosure url="https://github.com/surly-16/js-audio-feed/raw/refs/heads/main/AI%20Foundations%20and%20Implementation/media/004_ai_explainability_and_model_monitoring.mp3" length="4250733" type="audio/mpeg" /><description>Narrated episode: 004 Ai Explainability And Model Monitoring</description></item></channel></rss>